# -*- coding: utf-8 -*-
"""learn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zhH2vG1PVyQLtDDg1vSfiXY-UXUCZrl7
"""

"""# TPU Kernel Learning"""

import jax
from jax.experimental import pallas as pl
import jax.numpy as jnp
import numpy as np
import time
import datetime
import math
from jax.experimental.pallas import tpu as pltpu
import functools

## Hello world! (on TPU)


def add_vectors_kernel(a_ref, b_ref, o_ref):
  a = a_ref[...]
  b = b_ref[...]
  o_ref[...] = a + b

@jax.jit
def add_vectors(a: jax.Array, b: jax.Array) -> jax.Array:
  kernel = pl.pallas_call(
      add_vectors_kernel,
      out_shape=jax.ShapeDtypeStruct(a.shape, a.dtype)
  )
  return kernel(a, b)

a = jnp.arange(10)
b = jnp.arange(10)
start_time = datetime.datetime.now()
o = add_vectors(a, b)
end_time = datetime.datetime.now()
print(f'Time taken: {end_time - start_time}')
print(a, b, o)
if jnp.array_equal(o, a+b):
  print('Success!')
else:
  print('Fail!')

# Pallas takes in the program ID in kernel

# GPU Version

def show_program_id_kernel(o_ref):
  id = pl.program_id(0)
  o_ref[id] = id

def gpu_kernel_use_grid():
  def show_program_id(out_length: int)->jax.Array:
    kernel = pl.pallas_call(
        show_program_id_kernel,
        grid=(out_length),
        out_shape=jax.ShapeDtypeStruct((out_length,), jax.numpy.int32)
    )
    return kernel()

  print(show_program_id(16))


def tpu_kernel_use_grid():
  # TPUs distinguish between vector and scalar memory spaces and in this case
  # the output must be placed in scalar memory (TPUMemorySpace.SMEM)
  # since i is a scalar.
  def show_program_id(out_length: int)->jax.Array:
    kernel = pl.pallas_call(
        show_program_id_kernel,
        grid=(out_length),
        out_specs=jax.experimental.pallas.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM),
        out_shape=jax.ShapeDtypeStruct((out_length,), jax.numpy.int32)
    )
    return kernel()
  print(show_program_id(1024))


# gpu_kernel_use_grid()
tpu_kernel_use_grid()

"""## Grid and Idex

The grid in Pallas in different from `dim3 gridDim()` in cuda. The grid in cuda creates the slices in SMs, blocks. The grid in Pallas tries to abstract this process using a big tensor core with width usually being 128 values. It is actually a different form of nested loop.

`grid = (2,3)` means
```
for m in range(2):
  for n in range(3):
    ...
```

The TPU kernel is the loop body itself. We sent the sliced data through `pl.BlockSpec()`. It requires two things
1. The shape of sliced data.
2. The (i, j) mapping from the grid to the data slices.

What does `pl.BlockSpec()` do? It actually slices the input data (e.g. A) into small slices, then read (i, j) in grid to determine the loop indices for this body, then use `lambda i, j: (?, ?)` to get the slices input data.
For example,
```
  kernel = pl.pallas_call(
      ...
      grid=(2, 3),
      in_specs=[
          pl.BlockSpec((a.shape[0] // 2, a.shape[1]), lambda i, j: (i, 0)),
          pl.BlockSpec((b.shape[0], b.shape[1] // 3), lambda i, j: (0, j)),
      ],
      ...
)
```
 - We create a loop (2, 3)
   ```
     (0,0) (0,1) (0, 2)
     (1,0) (1,1) (1, 2)
   ```
 - We slice the input A to 2 slices on first dimension
 - We slice the input B to 3 slices on second dimension
 - `lambda i, j: (i, 0)` means we selects i for the first dimension of A slices and 0 for the second dimension of A slices, thus
 ```
 A_slices[0,0]  A_slices[0,0]  A_slices[0,0]
 A_slices[1,0]  A_slices[1,0]  A_slices[1,0]
 ```
 - Similarly, we have
 ```
   B_slices[0,0] B_slices[0,1] B_slices[0,2]
   B_slices[0,0] B_slices[0,1] B_slices[0,2]
 ```

So, for a nested loop
```
for i in range(2):
  for j in range(3):
    Do something on A slice, B slice
```

The output matrix is processed in the same way.
"""

#@title Test Grid and Index

def slices_for_invocation(x_shape: tuple[int, ...],
                          x_spec: pl.BlockSpec,
                          grid: tuple[int, ...],
                          invocation_indices: tuple[int, ...]) -> tuple[slice, ...]:
  assert len(invocation_indices) == len(grid)
  assert all(0 <= i < grid_size for i, grid_size in zip(invocation_indices, grid))
  block_indices = x_spec.index_map(*invocation_indices)
  assert len(x_shape) == len(x_spec.block_shape) == len(block_indices)
  elem_indices = []
  for x_size, block_size, block_idx in zip(x_shape, x_spec.block_shape, block_indices):
    start_idx = block_idx * block_size
    # At least one element of the block must be within bounds
    assert start_idx < x_size
    elem_indices.append(slice(start_idx, start_idx + block_size))
  return elem_indices

slices_for_invocation(x_shape=(100, 100),
                      x_spec = pl.BlockSpec((10, 20), lambda i, j: (i, j)),
                      grid = (10, 5),
                      invocation_indices = (2, 4))
slices_for_invocation(x_shape=(100, 100),
                      x_spec = pl.BlockSpec((10, 20), lambda i, j, k: (i, j)),
                      grid = (10, 5, 4),
                      invocation_indices = (2, 4, 0))

"""## MatMul Kernels"""

#@title Helper functions

import timeit

def matmul_flops(m: int, k: int, n: int):
  return 2 * m * k * n

def matmul_membw(m: int, k: int, n: int, dtype: jnp.dtype):
  return (m * k + k * n + m * n) * np.dtype(dtype).itemsize

def matmul_flops_intensity(m: int, k: int, n: int, dtype: jnp.dtype):
  flops = matmul_flops(m, k, n)
  membw = matmul_membw(m, k, n, dtype)
  return flops / membw

v5e_flops = 197e12
v5e_membw = 819e9
v5e_op_intensity = v5e_flops / v5e_membw  # ~240.5



def benchmark(f, ntrials: int = 100):
  def run(*args, **kwargs):
    # Compile function first
    jax.block_until_ready(f(*args, **kwargs))
    # Time function
    result = timeit.timeit(lambda: jax.block_until_ready(f(*args, **kwargs)),
                           number=ntrials)
    time = result / ntrials
    # print(f"Time: {time}")
    return time
  return run

def get_matmul_performance(m: int, k: int, n: int, dtype: np.dtype,
                   mm_func, **kwargs):
  k1, k2 = jax.random.split(jax.random.key(0), 2)
  a = jax.random.normal(k1, (m, k), dtype=dtype)
  b = jax.random.normal(k2, (k, n), dtype=dtype)
  time = benchmark(mm_func)(a, b, **kwargs)
  mm_flops = matmul_flops(m, k, n) / time
  return mm_flops, time


def analyze_matmul(m: int, k: int, n: int, dtype: np.dtype,
                   mm_func, **kwargs):
  custom_flops, time = get_matmul_performance(m, k, n, dtype, mm_func, **kwargs)
  print(f"----- {m} x {k} x {n} -----")
  print("Matmul time: ", time)
  print("Matmul GFLOP/s: ", custom_flops / 10**9)
  print(f"FLOP/s utilization: {custom_flops / v5e_flops * 100:.4f}%")
  xla_flops, _ = get_matmul_performance(m, k, n, dtype, jnp.matmul)
  print(f"Percentage of XLA FLOP/s: {custom_flops / xla_flops * 100:.4f}%")

#@title Kernel 0: XLA MatMul
print("================ XLA matmul ===================")
mm = jnp.matmul
analyze_matmul(1024, 1024, 1024, jnp.bfloat16, mm)
analyze_matmul(4096, 4096, 4096, jnp.bfloat16, mm)
analyze_matmul(8192, 8192, 8192, jnp.bfloat16, mm)

#@title Kernel V1: Naive version

def matmul_v1_naive_kernel(a_ref, b_ref, o_ref):
  o_ref[...] = a_ref[...] @ b_ref[...]

@jax.jit
def run_matmul_v1_naive(a: jax.Array, b: jax.Array):
  kernel = pl.pallas_call(
      matmul_v1_naive_kernel,
      out_shape=jax.ShapeDtypeStruct(a.shape, a.dtype)
  )
  return kernel(a, b)


k1, k2 = jax.random.split(jax.random.key(0))
a = jax.random.normal(k1, (1024, 1024), dtype=jax.numpy.float32)
b = jax.random.normal(k2, (1024, 1024), dtype=jax.numpy.float32)
if jnp.array_equal(run_matmul_v1_naive(a, b), jnp.matmul(a, b)):
  print('Success!')
else:
  print('Fail!')

analyze_matmul(m=1024, k =1024, n=1024, dtype=jnp.float32, mm_func=run_matmul_v1_naive)

#@title Kernel V2: Split Matrix by N in row or column


def matmul_v2_parallel_kernel(a_ref, b_ref, o_ref):
  o_ref[...] = a_ref[...] @ b_ref[...]

@functools.partial(jax.jit, static_argnames=['N'])
def run_matmul_v2_parallel(a: jax.Array, b: jax.Array, N: int):
  kernel = pl.pallas_call(
      matmul_v2_parallel_kernel,
      grid=(N, N),
      in_specs=[
          pl.BlockSpec((a.shape[0] // N, a.shape[1]), lambda i, j: (i, 0)),
          pl.BlockSpec((b.shape[0], b.shape[1] // N), lambda i, j: (0, j)),
      ],
      out_specs=pl.BlockSpec((a.shape[0] // N, b.shape[1] // N), lambda i, j: (i, j)),
      out_shape=jax.ShapeDtypeStruct((a.shape[0], b.shape[1]), a.dtype)
  )
  return kernel(a, b)


k1, k2 = jax.random.split(jax.random.key(0))
a = jax.random.normal(k1, (1024, 1024), dtype=jax.numpy.float32)
b = jax.random.normal(k2, (1024, 1024), dtype=jax.numpy.float32)
N = 8
if jnp.array_equal(run_matmul_v2_parallel(a, b, N), jnp.matmul(a, b)):
  print('Success!')
else:
  print('Fail!')

analyze_matmul(m=1024, k =1024, n=1024, dtype=jnp.float32, mm_func=run_matmul_v2_parallel, N=N)

#@title Kernel V3: Split Matrix by blocks

# Use 3D grid.
def matmul_v3_parallel_kernel(a_ref, b_ref, o_ref):
  @pl.when(pl.program_id(2) == 0)
  def init():
    o_ref[...] = jnp.zeros_like(o_ref)
  # Accumulates the multiplication for this block.
  o_ref[...] += a_ref[...] @ b_ref[...]

@functools.partial(jax.jit, static_argnames=['bm', 'bk', 'bn'])
def run_matmul_v3_parallel_kernel(
    a: jax.Array,
    b: jax.Array,
    *,
    bm: int = 128,
    bk: int = 128,
    bn: int = 128):
  m, k = a.shape
  _, n = b.shape
  assert k == b.shape[0]
  run_kernel = pl.pallas_call(
      matmul_v3_parallel_kernel,
      grid=(m // bm, n // bn, k // bk),
      in_specs = [
          pl.BlockSpec((bm, bk), lambda i, j, k : (i, k)),
          pl.BlockSpec((bk, bn), lambda i, j, k : (k, j)),
      ],
      out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),
      out_shape=jax.ShapeDtypeStruct((m, n), a.dtype),
  )
  return run_kernel(a, b)

m, k, n = 4096, 4096, 4096
k1, k2 = jax.random.split(jax.random.key(0), 2)
a = jax.random.normal(k1, (m, k), dtype=jnp.float32)
b = jax.random.normal(k2, (k, n), dtype=jnp.float32)

if jnp.array_equal(run_matmul_v3_parallel_kernel(a, b), jnp.matmul(a, b)):
  print('Success!')
else:
  print('Fail!')

analyze_matmul(m=m, k =k, n=n, dtype=jnp.float32, mm_func=run_matmul_v3_parallel_kernel)

#@title Kernel V4: Use BFLOAT16

def matmul_v4_bf16_kernel(a_ref, b_ref, o_ref, accu_ref, *, k_steps):
  @pl.when(pl.program_id(2) == 0)
  def init():
    accu_ref[...] = jnp.zeros_like(accu_ref)
  # Accumulates the multiplication for this block.
  # accu_ref += a_ref[...] @ b_ref[...]
  accu_ref[...] += jnp.dot(a_ref[...], b_ref[...], preferred_element_type=jnp.float32)
  @pl.when(pl.program_id(2) == k_steps-1)
  def update_result():
    o_ref[...] = accu_ref[...].astype(o_ref.dtype)

@functools.partial(jax.jit, static_argnames=['bm', 'bk', 'bn'])
def run_matmul_v4_bf16_kernel(
    a: jax.Array,
    b: jax.Array,
    *,
    bm: int = 128,
    bk: int = 128,
    bn: int = 128):
  m, k = a.shape
  _, n = b.shape
  assert k == b.shape[0]
  run_kernel = pl.pallas_call(
      # matmul_v4_bf16_kernel(k_steps = k),
      functools.partial(matmul_v4_bf16_kernel, k_steps=k // bk),
      grid=(m // bm, n // bn, k // bk),
      in_specs = [
          pl.BlockSpec((bm, bk), lambda i, j, k : (i, k)),
          pl.BlockSpec((bk, bn), lambda i, j, k : (k, j)),
      ],
      scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],
      out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),
      out_shape=jax.ShapeDtypeStruct((m, n), a.dtype),
      compiler_params=pltpu.TPUCompilerParams(
          dimension_semantics=("parallel", "parallel", "arbitrary")),
  )
  # run_kernel = pl.pallas_call(
  #     functools.partial(matmul_v4_bf16_kernel, k_steps=k // bk),
  #     grid_spec=pltpu.PrefetchScalarGridSpec(
  #       num_scalar_prefetch=0,
  #       in_specs=[
  #           pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)),
  #           pl.BlockSpec((bk, bn), lambda i, j, k: (k, j)),
  #       ],
  #       out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),
  #       scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],
  #       grid=(m // bm, n // bn, k // bk),
  #     ),
  #     out_shape=jax.ShapeDtypeStruct((m, n), a.dtype),
  #     compiler_params=pltpu.TPUCompilerParams(
  #         dimension_semantics=("parallel", "parallel", "arbitrary")),
  # )
  return run_kernel(a, b)

m, k, n = 4096, 4096, 4096
bm, bk, bn = 128, 128, 128
k1, k2 = jax.random.split(jax.random.key(0), 2)
a = jax.random.normal(k1, (m, k), dtype=jnp.bfloat16)
b = jax.random.normal(k2, (k, n), dtype=jnp.bfloat16)
correct = jnp.matmul(a, b)
wrong = run_matmul_v4_bf16_kernel(a, b, bm=bm, bk=bk, bn=bn)

# for row in range(min(correct.shape[0], wrong.shape[0])):
#   for col in range(min(correct.shape[1], wrong.shape[1])):
#     if correct[row, col] != wrong[row, col]:
#       print(row, col, correct[row, col], wrong[row, col])
#       break

if jnp.allclose(run_matmul_v4_bf16_kernel(a, b, bm=bm, bk=bk, bn=bn), jnp.matmul(a, b), atol=2):
  print('Success!')
else:
  print('Fail!')

analyze_matmul(m=m, k=k, n=n, dtype=jnp.bfloat16, mm_func=run_matmul_v4_bf16_kernel, bm=bm, bk=bk, bn=bn)

#@title Kernel V5: Turn to bigger block

def matmul_v5_largeblock_kernel(a_ref, b_ref, o_ref, accu_ref, *, k_steps):
  @pl.when(pl.program_id(2) == 0)
  def init():
    accu_ref[...] = jnp.zeros_like(accu_ref)
  # Accumulates the multiplication for this block.
  # accu_ref += a_ref[...] @ b_ref[...]
  accu_ref[...] += jnp.dot(a_ref[...], b_ref[...], preferred_element_type=jnp.float32)
  @pl.when(pl.program_id(2) == k_steps-1)
  def update_result():
    o_ref[...] = accu_ref[...].astype(o_ref.dtype)


def run_matmul_v5_largeblock_kernel(
    a: jax.Array,
    b: jax.Array,
    *,
    bm: int = 128,
    bk: int = 128,
    bn: int = 128):
  m, k = a.shape
  _, n = b.shape
  assert k == b.shape[0]
  run_kernel = pl.pallas_call(
      functools.partial(matmul_v4_bf16_kernel, k_steps=k // bk),
      grid=(m // bm, n // bn, k // bk),
      in_specs = [
          pl.BlockSpec((bm, bk), lambda i, j, k : (i, k)),
          pl.BlockSpec((bk, bn), lambda i, j, k : (k, j)),
      ],
      scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],
      out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),
      out_shape=jax.ShapeDtypeStruct((m, n), a.dtype),
      compiler_params=pltpu.TPUCompilerParams(
          dimension_semantics=("parallel", "parallel", "arbitrary")),
  )
  # run_kernel = pl.pallas_call(
  #     functools.partial(matmul_v4_bf16_kernel, k_steps=k // bk),
  #     grid_spec=pltpu.PrefetchScalarGridSpec(
  #       num_scalar_prefetch=0,
  #       in_specs=[
  #           pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)),
  #           pl.BlockSpec((bk, bn), lambda i, j, k: (k, j)),
  #       ],
  #       out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),
  #       scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],
  #       grid=(m // bm, n // bn, k // bk),
  #     ),
  #     out_shape=jax.ShapeDtypeStruct((m, n), a.dtype),
  #     compiler_params=pltpu.TPUCompilerParams(
  #         dimension_semantics=("parallel", "parallel", "arbitrary")),
  # )
  return run_kernel(a, b)

m, k, n = 8192, 8192, 8192
# bm, bk, bn = 128, 128, 128
bm, bk, bn = 512, 1024, 1024
dtype = jnp.bfloat16
k1, k2 = jax.random.split(jax.random.key(0), 2)
# a = jax.random.normal(k1, (m, k), dtype=dtype)
# b = jax.random.normal(k2, (k, n), dtype=dtype)
a = jnp.ones((m, k), dtype=dtype)
b = jnp.ones((k, n), dtype=dtype)

if jnp.allclose(run_matmul_v5_largeblock_kernel(a, b, bm=bm, bk=bk, bn=bn), jnp.matmul(a, b), atol=2):
  print('Success!')
else:
  print('Fail!')
analyze_matmul(m=m, k=k, n=n, dtype=dtype, mm_func=run_matmul_v4_bf16_kernel,
               bm=bm, bk=bk, bn=bn)

#@title Kernel V6: Use INT8

def matmul_v6_int8_kernel(a_ref, b_ref, o_ref, accu_ref, *, k_steps):
  @pl.when(pl.program_id(2) == 0)
  def init():
    accu_ref[...] = jnp.zeros_like(accu_ref)
  # Accumulates the multiplication for this block.
  # accu_ref += a_ref[...] @ b_ref[...]
  accu_ref[...] += jnp.dot(a_ref[...], b_ref[...], preferred_element_type=jnp.float32)
  @pl.when(pl.program_id(2) == k_steps-1)
  def update_result():
    o_ref[...] = accu_ref[...].astype(o_ref.dtype)


def run_matmul_v6_int8_kernel(
    a: jax.Array,
    b: jax.Array,
    *,
    bm: int = 128,
    bk: int = 128,
    bn: int = 128):
  m, k = a.shape
  _, n = b.shape
  assert k == b.shape[0]
  run_kernel = pl.pallas_call(
      functools.partial(matmul_v6_int8_kernel, k_steps=k // bk),
      grid=(m // bm, n // bn, k // bk),
      in_specs = [
          pl.BlockSpec((bm, bk), lambda i, j, k : (i, k)),
          pl.BlockSpec((bk, bn), lambda i, j, k : (k, j)),
      ],
      scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],
      out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),
      out_shape=jax.ShapeDtypeStruct((m, n), a.dtype),
      compiler_params=pltpu.TPUCompilerParams(
          dimension_semantics=("parallel", "parallel", "arbitrary")),
  )
  # run_kernel = pl.pallas_call(
  #     functools.partial(matmul_v4_bf16_kernel, k_steps=k // bk),
  #     grid_spec=pltpu.PrefetchScalarGridSpec(
  #       num_scalar_prefetch=0,
  #       in_specs=[
  #           pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)),
  #           pl.BlockSpec((bk, bn), lambda i, j, k: (k, j)),
  #       ],
  #       out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),
  #       scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)],
  #       grid=(m // bm, n // bn, k // bk),
  #     ),
  #     out_shape=jax.ShapeDtypeStruct((m, n), a.dtype),
  #     compiler_params=pltpu.TPUCompilerParams(
  #         dimension_semantics=("parallel", "parallel", "arbitrary")),
  # )
  return run_kernel(a, b)

m, k, n = 8192, 8192, 8192
# bm, bk, bn = 128, 128, 128
bm, bk, bn = 512, 1024, 1024
dtype = jnp.bfloat16
k1, k2 = jax.random.split(jax.random.key(0), 2)
# a = jax.random.normal(k1, (m, k), dtype=dtype)
# b = jax.random.normal(k2, (k, n), dtype=dtype)
a = jnp.ones((m, k), dtype=dtype)
b = jnp.ones((k, n), dtype=dtype)

if jnp.allclose(run_matmul_v6_int8_kernel(a, b, bm=bm, bk=bk, bn=bn), jnp.matmul(a, b), atol=2):
  print('Success!')
else:
  print('Fail!')
analyze_matmul(m=m, k=k, n=n, dtype=dtype, mm_func=run_matmul_v6_int8_kernel,
               bm=bm, bk=bk, bn=bn)

"""## MatMul with Batch processing using vmap"""

#@title Batch processing using vmap, with activation function


def matmul_batch_parallel_kernel(a_ref, b_ref, o_ref, *, activation):
  o_ref[...] = activation(a_ref[...] @ b_ref[...])

@functools.partial(jax.jit, static_argnames=['N', 'activation'])
def run_matmul_batch_parallel(a: jax.Array, b: jax.Array, N: int, activation):
  kernel = pl.pallas_call(
      functools.partial(matmul_batch_parallel_kernel, activation=activation),
      grid=(N, N),
      in_specs=[
          pl.BlockSpec((a.shape[0] // N, a.shape[1]), lambda i, j: (i, 0)),
          pl.BlockSpec((b.shape[0], b.shape[1] // N), lambda i, j: (0, j)),
      ],
      out_specs=pl.BlockSpec((a.shape[0] // N, b.shape[1] // N), lambda i, j: (i, j)),
      out_shape=jax.ShapeDtypeStruct((a.shape[0], b.shape[1]), a.dtype)
  )
  return kernel(a, b)

def test_matmul_v3_parallel(a: jax.Array, b: jax.Array, N: int):
  start_time = datetime.datetime.now()
  out = jax.vmap(functools.partial(run_matmul_batch_parallel, N = N, activation=jax.nn.relu), in_axes=(0, 0))(a, b)
  end_time = datetime.datetime.now()
  elapes_seconds = (end_time - start_time).total_seconds()
  print(f'Time taken: {end_time - start_time}, performance is {2 * math.prod(a.shape) * b.shape[-1] * a.dtype.itemsize * 8 / elapes_seconds / 10**9} GFLOPS')
  out_correct_value = jax.nn.relu(jax.vmap(jnp.matmul)(a, b))
  if jnp.array_equal(out, out_correct_value):
    print('Success!')
  else:
    print('Fail!')


k1, k2 = jax.random.split(jax.random.key(0))
a = jax.random.normal(k1, (4, 1024, 1024), dtype=jax.numpy.float32)
b = jax.random.normal(k2, (4, 1024, 1024), dtype=jax.numpy.float32)
test_matmul_v3_parallel(a, b, 8)

